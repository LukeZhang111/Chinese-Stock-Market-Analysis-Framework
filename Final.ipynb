{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Chinese Stock Markets: A Comprehensive Study\n",
    "## Data Collection and Analysis System\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Dependencies\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Financial analysis\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "yf.pdr_override()\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Database management\n",
    "import sqlite3\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"Setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Setup\n",
    "\n",
    "We'll create a SQLite database to store our financial data. The database will include tables for:\n",
    "1. Daily stock prices and volumes\n",
    "2. Company information\n",
    "3. Market indices\n",
    "4. Sector classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseManager:\n",
    "    def __init__(self, db_name='chinese_stocks.db'):\n",
    "        \"\"\"Initialize database connection and create tables\"\"\"\n",
    "        self.conn = sqlite3.connect(db_name)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.create_tables()\n",
    "    \n",
    "    def create_tables(self):\n",
    "        \"\"\"Create all necessary database tables\"\"\"\n",
    "        # Stock price data table\n",
    "        self.cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS stock_data (\n",
    "                date TEXT,\n",
    "                symbol TEXT,\n",
    "                open REAL,\n",
    "                high REAL,\n",
    "                low REAL,\n",
    "                close REAL,\n",
    "                volume REAL,\n",
    "                adj_close REAL,\n",
    "                market_cap REAL,\n",
    "                PRIMARY KEY (date, symbol)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Company information table\n",
    "        self.cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS company_info (\n",
    "                symbol TEXT PRIMARY KEY,\n",
    "                name TEXT,\n",
    "                sector TEXT,\n",
    "                industry TEXT,\n",
    "                exchange TEXT,\n",
    "                description TEXT\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Market indices table\n",
    "        self.cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS market_indices (\n",
    "                date TEXT,\n",
    "                index_code TEXT,\n",
    "                value REAL,\n",
    "                change_pct REAL,\n",
    "                PRIMARY KEY (date, index_code)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        self.conn.commit()\n",
    "    \n",
    "    def store_stock_data(self, df, symbol):\n",
    "        \"\"\"Store stock price data in database\"\"\"\n",
    "        df_copy = df.reset_index()\n",
    "        df_copy['symbol'] = symbol\n",
    "        df_copy.to_sql('stock_data', self.conn, if_exists='append', index=False)\n",
    "    \n",
    "    def get_stock_data(self, symbol, start_date, end_date):\n",
    "        \"\"\"Retrieve stock data from database\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM stock_data\n",
    "        WHERE symbol = ? AND date BETWEEN ? AND ?\n",
    "        ORDER BY date\n",
    "        \"\"\"\n",
    "        return pd.read_sql_query(query, self.conn, params=(symbol, start_date, end_date))\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection\"\"\"\n",
    "        self.conn.close()\n",
    "\n",
    "# Initialize database\n",
    "db = DatabaseManager()\n",
    "print(\"Database initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Universe Definition\n",
    "\n",
    "Define the list of stocks we'll analyze, including major companies from both SSE and SZSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stock universes\n",
    "stock_universe = {\n",
    "    'SSE': {  # Shanghai Stock Exchange\n",
    "        '600519.SS': 'Kweichow Moutai',\n",
    "        '601318.SS': 'Ping An Insurance',\n",
    "        '600036.SS': 'China Merchants Bank',\n",
    "        '601398.SS': 'ICBC',\n",
    "        '600276.SS': 'Jiangsu Hengrui Medicine'\n",
    "    },\n",
    "    'SZSE': {  # Shenzhen Stock Exchange\n",
    "        '000858.SZ': 'Wuliangye Yibin',\n",
    "        '000333.SZ': 'Midea Group',\n",
    "        '000651.SZ': 'Gree Electric',\n",
    "        '000002.SZ': 'Vanke A',\n",
    "        '002594.SZ': 'BYD Company'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a DataFrame with stock information\n",
    "stock_info = pd.DataFrame([\n",
    "    {'symbol': symbol, 'name': name, 'exchange': exchange}\n",
    "    for exchange, stocks in stock_universe.items()\n",
    "    for symbol, name in stocks.items()\n",
    "])\n",
    "\n",
    "print(\"Stock universe defined:\")\n",
    "display(stock_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Settings\n",
    "\n",
    "Define global settings and parameters for our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Date ranges\n",
    "    END_DATE = datetime.now()\n",
    "    START_DATE = END_DATE - timedelta(days=365*2)  # 2 years of data\n",
    "    \n",
    "    # Technical analysis parameters\n",
    "    MOVING_AVERAGES = [20, 50, 200]  # Days for moving averages\n",
    "    VOLATILITY_WINDOW = 30  # Days for volatility calculation\n",
    "    \n",
    "    # Visualization settings\n",
    "    PLOT_STYLE = 'seaborn'\n",
    "    PLOT_FIGSIZE = (15, 8)\n",
    "    \n",
    "    # Analysis parameters\n",
    "    CORRELATION_THRESHOLD = 0.7\n",
    "    ZSCORE_THRESHOLD = 2.0\n",
    "    \n",
    "    # Performance metrics\n",
    "    RISK_FREE_RATE = 0.03  # 3% annual risk-free rate\n",
    "\n",
    "print(\"Configuration settings initialized!\")\n",
    "print(f\"Analysis period: {Config.START_DATE.date()} to {Config.END_DATE.date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection System\n",
    "\n",
    "We'll create a robust system to fetch historical data for Chinese stocks, handle missing data, and perform initial data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollector:\n",
    "    def __init__(self, db_manager):\n",
    "        \"\"\"Initialize data collector with database manager\"\"\"\n",
    "        self.db = db_manager\n",
    "        self.failed_symbols = []\n",
    "    \n",
    "    def fetch_stock_data(self, symbol, start_date, end_date):\n",
    "        \"\"\"Fetch stock data from Yahoo Finance\"\"\"\n",
    "        try:\n",
    "            data = yf.download(symbol, start=start_date, end=end_date, progress=False)\n",
    "            if len(data) > 0:\n",
    "                return data\n",
    "            else:\n",
    "                self.failed_symbols.append(symbol)\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {str(e)}\")\n",
    "            self.failed_symbols.append(symbol)\n",
    "            return None\n",
    "    \n",
    "    def fetch_multiple_stocks(self, symbols, start_date, end_date):\n",
    "        \"\"\"Fetch data for multiple stocks\"\"\"\n",
    "        all_data = {}\n",
    "        for symbol in symbols:\n",
    "            print(f\"Fetching data for {symbol}...\")\n",
    "            data = self.fetch_stock_data(symbol, start_date, end_date)\n",
    "            if data is not None:\n",
    "                all_data[symbol] = data\n",
    "                self.db.store_stock_data(data, symbol)\n",
    "        return all_data\n",
    "\n",
    "# Initialize data collector\n",
    "collector = DataCollector(db)\n",
    "\n",
    "# Fetch data for all stocks\n",
    "all_symbols = [symbol for stocks in stock_universe.values() for symbol in stocks.keys()]\n",
    "stock_data = collector.fetch_multiple_stocks(all_symbols, Config.START_DATE, Config.END_DATE)\n",
    "\n",
    "print(f\"\\nData collection completed!\")\n",
    "print(f\"Successfully collected data for {len(stock_data)} stocks\")\n",
    "if collector.failed_symbols:\n",
    "    print(f\"Failed to collect data for {len(collector.failed_symbols)} stocks: {collector.failed_symbols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Now let's clean and preprocess the collected data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, stock_data):\n",
    "        \"\"\"Initialize preprocessor with collected stock data\"\"\"\n",
    "        self.stock_data = stock_data\n",
    "        self.processed_data = {}\n",
    "    \n",
    "    def handle_missing_values(self, df):\n",
    "        \"\"\"Handle missing values in the dataset\"\"\"\n",
    "        # Forward fill price data\n",
    "        df['Close'] = df['Close'].ffill()\n",
    "        df['Open'] = df['Open'].ffill()\n",
    "        df['High'] = df['High'].ffill()\n",
    "        df['Low'] = df['Low'].ffill()\n",
    "        \n",
    "        # Fill volume with 0\n",
    "        df['Volume'] = df['Volume'].fillna(0)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def calculate_returns(self, df):\n",
    "        \"\"\"Calculate daily and cumulative returns\"\"\"\n",
    "        df['Daily_Return'] = df['Close'].pct_change()\n",
    "        df['Cumulative_Return'] = (1 + df['Daily_Return']).cumprod()\n",
    "        return df\n",
    "    \n",
    "    def calculate_volatility(self, df):\n",
    "        \"\"\"Calculate rolling volatility\"\"\"\n",
    "        df['Volatility'] = df['Daily_Return'].rolling(window=Config.VOLATILITY_WINDOW).std() * np.sqrt(252)\n",
    "        return df\n",
    "    \n",
    "    def calculate_moving_averages(self, df):\n",
    "        \"\"\"Calculate various moving averages\"\"\"\n",
    "        for period in Config.MOVING_AVERAGES:\n",
    "            df[f'MA_{period}'] = df['Close'].rolling(window=period).mean()\n",
    "        return df\n",
    "    \n",
    "    def process_all_stocks(self):\n",
    "        \"\"\"Process all stocks in the dataset\"\"\"\n",
    "        for symbol, data in self.stock_data.items():\n",
    "            df = data.copy()\n",
    "            \n",
    "            # Apply all preprocessing steps\n",
    "            df = self.handle_missing_values(df)\n",
    "            df = self.calculate_returns(df)\n",
    "            df = self.calculate_volatility(df)\n",
    "            df = self.calculate_moving_averages(df)\n",
    "            \n",
    "            self.processed_data[symbol] = df\n",
    "        \n",
    "        return self.processed_data\n",
    "\n",
    "# Process the data\n",
    "preprocessor = DataPreprocessor(stock_data)\n",
    "processed_data = preprocessor.process_all_stocks()\n",
    "\n",
    "# Display sample of processed data\n",
    "sample_symbol = list(processed_data.keys())[0]\n",
    "print(f\"\\nSample of processed data for {sample_symbol}:\")\n",
    "display(processed_data[sample_symbol].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Checks\n",
    "\n",
    "Let's perform some basic data quality checks on our processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_data_quality_checks(processed_data):\n",
    "    \"\"\"Perform various data quality checks\"\"\"\n",
    "    quality_report = {}\n",
    "    \n",
    "    for symbol, data in processed_data.items():\n",
    "        report = {\n",
    "            'total_rows': len(data),\n",
    "            'missing_values': data.isnull().sum().to_dict(),\n",
    "            'negative_prices': (data['Close'] < 0).sum(),\n",
    "            'zero_prices': (data['Close'] == 0).sum(),\n",
    "            'date_range': f\"{data.index.min()} to {data.index.max()}\",\n",
    "            'trading_days': len(data),\n",
    "            'price_range': f\"{data['Close'].min():.2f} - {data['Close'].max():.2f}\"\n",
    "        }\n",
    "        \n",
    "        # Check for extreme returns\n",
    "        mean_return = data['Daily_Return'].mean()\n",
    "        std_return = data['Daily_Return'].std()\n",
    "        extreme_returns = data['Daily_Return'][abs(data['Daily_Return'] - mean_return) > 3 * std_return]\n",
    "        report['extreme_returns_count'] = len(extreme_returns)\n",
    "        \n",
    "        quality_report[symbol] = report\n",
    "    \n",
    "    return pd.DataFrame(quality_report).T\n",
    "\n",
    "# Run quality checks\n",
    "quality_report = perform_data_quality_checks(processed_data)\n",
    "print(\"Data Quality Report:\")\n",
    "display(quality_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Summary\n",
    "\n",
    "Let's create a summary of our processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_summary(processed_data):\n",
    "    \"\"\"Create summary statistics for all stocks\"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    for symbol, data in processed_data.items():\n",
    "        summary = {\n",
    "            'Symbol': symbol,\n",
    "            'Start_Date': data.index.min(),\n",
    "            'End_Date': data.index.max(),\n",
    "            'Avg_Price': data['Close'].mean(),\n",
    "            'Avg_Volume': data['Volume'].mean(),\n",
    "            'Avg_Daily_Return': data['Daily_Return'].mean() * 100,\n",
    "            'Volatility': data['Volatility'].mean() * 100,\n",
    "            'Max_Drawdown': ((data['Close'] / data['Close'].cummax() - 1).min() * 100),\n",
    "            'Total_Return': ((data['Close'][-1] / data['Close'][0] - 1) * 100)\n",
    "        }\n",
    "        summary_data.append(summary)\n",
    "    \n",
    "    return pd.DataFrame(summary_data).round(2)\n",
    "\n",
    "# Create and display summary\n",
    "data_summary = create_data_summary(processed_data)\n",
    "print(\"Data Summary:\")\n",
    "display(data_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Processed Data\n",
    "\n",
    "Finally, let's save our processed data back to the database for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(processed_data, db):\n",
    "    \"\"\"Save processed data to database\"\"\"\n",
    "    for symbol, data in processed_data.items():\n",
    "        # Prepare data for storage\n",
    "        df_to_save = data.reset_index()\n",
    "        df_to_save['symbol'] = symbol\n",
    "        \n",
    "        # Store in database\n",
    "        df_to_save.to_sql('processed_stock_data', db.conn, if_exists='append', index=False)\n",
    "    \n",
    "    print(\"Processed data saved successfully!\")\n",
    "\n",
    "# Save the processed data\n",
    "save_processed_data(processed_data, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Analysis Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalAnalysis:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def calculate_RSI(self, df, periods=14):\n",
    "        \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "        delta = df['Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=periods).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=periods).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    def calculate_MACD(self, df, short_period=12, long_period=26, signal_period=9):\n",
    "        \"\"\"Calculate MACD (Moving Average Convergence Divergence)\"\"\"\n",
    "        short_ema = df['Close'].ewm(span=short_period, adjust=False).mean()\n",
    "        long_ema = df['Close'].ewm(span=long_period, adjust=False).mean()\n",
    "        macd = short_ema - long_ema\n",
    "        signal = macd.ewm(span=signal_period, adjust=False).mean()\n",
    "        return macd, signal\n",
    "    \n",
    "    def calculate_bollinger_bands(self, df, window=20, num_std=2):\n",
    "        \"\"\"Calculate Bollinger Bands\"\"\"\n",
    "        rolling_mean = df['Close'].rolling(window=window).mean()\n",
    "        rolling_std = df['Close'].rolling(window=window).std()\n",
    "        upper_band = rolling_mean + (rolling_std * num_std)\n",
    "        lower_band = rolling_mean - (rolling_std * num_std)\n",
    "        return rolling_mean, upper_band, lower_band\n",
    "    \n",
    "    def add_technical_indicators(self):\n",
    "        \"\"\"Add all technical indicators to the data\"\"\"\n",
    "        for symbol, df in self.data.items():\n",
    "            # RSI\n",
    "            df['RSI'] = self.calculate_RSI(df)\n",
    "            \n",
    "            # MACD\n",
    "            df['MACD'], df['MACD_Signal'] = self.calculate_MACD(df)\n",
    "            df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
    "            \n",
    "            # Bollinger Bands\n",
    "            df['BB_Middle'], df['BB_Upper'], df['BB_Lower'] = self.calculate_bollinger_bands(df)\n",
    "            \n",
    "            self.data[symbol] = df\n",
    "        \n",
    "        return self.data\n",
    "\n",
    "class MarketVisualizer:\n",
    "    def __init__(self, analysis_data):\n",
    "        self.data = analysis_data\n",
    "    \n",
    "    def plot_price_with_indicators(self, symbol):\n",
    "        \"\"\"Create a comprehensive price chart with technical indicators\"\"\"\n",
    "        df = self.data[symbol]\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        gs = fig.add_gridspec(3, 1, height_ratios=[2, 1, 1])\n",
    "        \n",
    "        # Price and Bollinger Bands\n",
    "        ax1 = fig.add_subplot(gs[0])\n",
    "        ax1.plot(df.index, df['Close'], label='Price', color='blue')\n",
    "        ax1.plot(df.index, df['BB_Upper'], '--', label='Upper BB', color='gray', alpha=0.5)\n",
    "        ax1.plot(df.index, df['BB_Middle'], '--', label='Middle BB', color='gray', alpha=0.5)\n",
    "        ax1.plot(df.index, df['BB_Lower'], '--', label='Lower BB', color='gray', alpha=0.5)\n",
    "        ax1.fill_between(df.index, df['BB_Upper'], df['BB_Lower'], alpha=0.1)\n",
    "        ax1.set_title(f'{symbol} Price and Technical Indicators')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # MACD\n",
    "        ax2 = fig.add_subplot(gs[1])\n",
    "        ax2.plot(df.index, df['MACD'], label='MACD', color='blue')\n",
    "        ax2.plot(df.index, df['MACD_Signal'], label='Signal', color='orange')\n",
    "        ax2.bar(df.index, df['MACD_Histogram'], label='Histogram', color='gray', alpha=0.3)\n",
    "        ax2.set_title('MACD')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        # RSI\n",
    "        ax3 = fig.add_subplot(gs[2])\n",
    "        ax3.plot(df.index, df['RSI'], label='RSI', color='purple')\n",
    "        ax3.axhline(y=70, color='r', linestyle='--', alpha=0.5)\n",
    "        ax3.axhline(y=30, color='g', linestyle='--', alpha=0.5)\n",
    "        ax3.set_title('RSI')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_market_comparison(self):\n",
    "        \"\"\"Create a comparison plot of all stocks\"\"\"\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        for symbol, df in self.data.items():\n",
    "            normalized_price = df['Close'] / df['Close'].iloc[0] * 100\n",
    "            plt.plot(df.index, normalized_price, label=symbol)\n",
    "        \n",
    "        plt.title('Normalized Price Comparison (Base=100)')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Normalized Price')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_correlation_heatmap(self):\n",
    "        \"\"\"Create a correlation heatmap of stock returns\"\"\"\n",
    "        returns_data = {}\n",
    "        for symbol, df in self.data.items():\n",
    "            returns_data[symbol] = df['Daily_Return']\n",
    "        \n",
    "        returns_df = pd.DataFrame(returns_data)\n",
    "        correlation_matrix = returns_df.corr()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "        plt.title('Stock Returns Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "class VolumeAnalysis:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def analyze_volume_trends(self, symbol):\n",
    "        \"\"\"Analyze trading volume trends\"\"\"\n",
    "        df = self.data[symbol]\n",
    "        \n",
    "        # Calculate volume metrics\n",
    "        df['Volume_MA_10'] = df['Volume'].rolling(window=10).mean()\n",
    "        df['Volume_MA_30'] = df['Volume'].rolling(window=30).mean()\n",
    "        \n",
    "        # Plot volume analysis\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "        \n",
    "        # Price and volume\n",
    "        ax1.plot(df.index, df['Close'], color='blue')\n",
    "        ax1_twin = ax1.twinx()\n",
    "        ax1_twin.bar(df.index, df['Volume'], alpha=0.3, color='gray')\n",
    "        ax1.set_title(f'{symbol} Price and Volume')\n",
    "        ax1.set_ylabel('Price')\n",
    "        ax1_twin.set_ylabel('Volume')\n",
    "        \n",
    "        # Volume moving averages\n",
    "        ax2.plot(df.index, df['Volume_MA_10'], label='10-day MA', color='blue')\n",
    "        ax2.plot(df.index, df['Volume_MA_30'], label='30-day MA', color='orange')\n",
    "        ax2.bar(df.index, df['Volume'], alpha=0.3, color='gray')\n",
    "        ax2.set_title('Volume Analysis')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate volume statistics\n",
    "        volume_stats = {\n",
    "            'Average Daily Volume': df['Volume'].mean(),\n",
    "            'Max Daily Volume': df['Volume'].max(),\n",
    "            'Volume Volatility': df['Volume'].std() / df['Volume'].mean(),\n",
    "            'Volume Trend': (df['Volume_MA_10'][-1] / df['Volume_MA_30'][-1] - 1) * 100\n",
    "        }\n",
    "        \n",
    "        return pd.Series(volume_stats)\n",
    "    \n",
    "    def analyze_all_stocks(self):\n",
    "        \"\"\"Analyze volume trends for all stocks\"\"\"\n",
    "        all_stats = {}\n",
    "        for symbol in self.data.keys():\n",
    "            all_stats[symbol] = self.analyze_volume_trends(symbol)\n",
    "        \n",
    "        return pd.DataFrame(all_stats).T\n",
    "\n",
    "class PatternAnalysis:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def identify_trends(self, df, window=20):\n",
    "        \"\"\"Identify market trends using moving averages\"\"\"\n",
    "        df['Trend'] = 'Neutral'\n",
    "        df.loc[df['Close'] > df[f'MA_{window}'], 'Trend'] = 'Uptrend'\n",
    "        df.loc[df['Close'] < df[f'MA_{window}'], 'Trend'] = 'Downtrend'\n",
    "        return df\n",
    "    \n",
    "    def detect_swings(self, df, threshold=0.02):\n",
    "        \"\"\"Detect major price swings\"\"\"\n",
    "        df['Price_Change'] = df['Close'].pct_change()\n",
    "        df['Swing'] = 'None'\n",
    "        \n",
    "        # Identify significant up and down moves\n",
    "        df.loc[df['Price_Change'] > threshold, 'Swing'] = 'Up'\n",
    "        df.loc[df['Price_Change'] < -threshold, 'Swing'] = 'Down'\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def find_support_resistance(self, df, window=20):\n",
    "        \"\"\"Find potential support and resistance levels\"\"\"\n",
    "        df['Support'] = df['Low'].rolling(window=window, center=True).min()\n",
    "        df['Resistance'] = df['High'].rolling(window=window, center=True).max()\n",
    "        \n",
    "        # Identify key levels\n",
    "        price_range = df['High'].max() - df['Low'].min()\n",
    "        threshold = price_range * 0.02  # 2% of total range\n",
    "        \n",
    "        support_levels = []\n",
    "        resistance_levels = []\n",
    "        \n",
    "        for level in df['Support'].dropna().unique():\n",
    "            if sum(abs(df['Low'] - level) < threshold) > window/2:\n",
    "                support_levels.append(level)\n",
    "        \n",
    "        for level in df['Resistance'].dropna().unique():\n",
    "            if sum(abs(df['High'] - level) < threshold) > window/2:\n",
    "                resistance_levels.append(level)\n",
    "                \n",
    "        return support_levels, resistance_levels\n",
    "    \n",
    "    def analyze_patterns(self, symbol):\n",
    "        \"\"\"Perform comprehensive pattern analysis for a stock\"\"\"\n",
    "        df = self.data[symbol].copy()\n",
    "        \n",
    "        # Identify trends\n",
    "        df = self.identify_trends(df)\n",
    "        \n",
    "        # Detect swings\n",
    "        df = self.detect_swings(df)\n",
    "        \n",
    "        # Find support and resistance\n",
    "        support_levels, resistance_levels = self.find_support_resistance(df)\n",
    "        \n",
    "        # Visualize patterns\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "        \n",
    "        # Price with support and resistance\n",
    "        ax1.plot(df.index, df['Close'], label='Price')\n",
    "        for level in support_levels:\n",
    "            ax1.axhline(y=level, color='g', linestyle='--', alpha=0.5)\n",
    "        for level in resistance_levels:\n",
    "            ax1.axhline(y=level, color='r', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        ax1.set_title(f'{symbol} Price with Support/Resistance Levels')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Trend analysis\n",
    "        colors = {'Uptrend': 'g', 'Downtrend': 'r', 'Neutral': 'gray'}\n",
    "        for trend in colors:\n",
    "            mask = df['Trend'] == trend\n",
    "            ax2.scatter(df[mask].index, df[mask]['Close'], \n",
    "                       c=colors[trend], label=trend, alpha=0.5)\n",
    "        \n",
    "        ax2.set_title('Trend Analysis')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate pattern statistics\n",
    "        pattern_stats = {\n",
    "            'Uptrend_Days': sum(df['Trend'] == 'Uptrend'),\n",
    "            'Downtrend_Days': sum(df['Trend'] == 'Downtrend'),\n",
    "            'Neutral_Days': sum(df['Trend'] == 'Neutral'),\n",
    "            'Major_Swings_Up': sum(df['Swing'] == 'Up'),\n",
    "            'Major_Swings_Down': sum(df['Swing'] == 'Down'),\n",
    "            'Support_Levels': len(support_levels),\n",
    "            'Resistance_Levels': len(resistance_levels)\n",
    "        }\n",
    "        \n",
    "        return pd.Series(pattern_stats)\n",
    "    \n",
    "    def analyze_all_stocks(self):\n",
    "        \"\"\"Analyze patterns for all stocks\"\"\"\n",
    "        pattern_stats = {}\n",
    "        for symbol in self.data.keys():\n",
    "            print(f\"\\nAnalyzing patterns for {symbol}...\")\n",
    "            pattern_stats[symbol] = self.analyze_patterns(symbol)\n",
    "        \n",
    "        return pd.DataFrame(pattern_stats).T\n",
    "\n",
    "class RiskAnalysis:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def calculate_risk_metrics(self, df):\n",
    "        \"\"\"Calculate various risk metrics\"\"\"\n",
    "        # Volatility metrics\n",
    "        daily_returns = df['Daily_Return']\n",
    "        annual_volatility = daily_returns.std() * np.sqrt(252)\n",
    "        \n",
    "        # Downside risk\n",
    "        downside_returns = daily_returns[daily_returns < 0]\n",
    "        downside_volatility = downside_returns.std() * np.sqrt(252)\n",
    "        \n",
    "        # Value at Risk (VaR)\n",
    "        var_95 = np.percentile(daily_returns, 5)\n",
    "        var_99 = np.percentile(daily_returns, 1)\n",
    "        \n",
    "        # Maximum drawdown\n",
    "        rolling_max = df['Close'].expanding().max()\n",
    "        drawdowns = df['Close']/rolling_max - 1\n",
    "        max_drawdown = drawdowns.min()\n",
    "        \n",
    "        # Sharpe ratio (assuming risk-free rate of 3%)\n",
    "        excess_returns = daily_returns - Config.RISK_FREE_RATE/252\n",
    "        sharpe_ratio = np.sqrt(252) * excess_returns.mean() / excess_returns.std()\n",
    "        \n",
    "        return pd.Series({\n",
    "            'Annual_Volatility': annual_volatility,\n",
    "            'Downside_Volatility': downside_volatility,\n",
    "            'VaR_95': var_95,\n",
    "            'VaR_99': var_99,\n",
    "            'Max_Drawdown': max_drawdown,\n",
    "            'Sharpe_Ratio': sharpe_ratio\n",
    "        })\n",
    "    \n",
    "    def plot_risk_analysis(self, symbol):\n",
    "        \"\"\"Create risk analysis visualizations\"\"\"\n",
    "        df = self.data[symbol]\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Rolling volatility\n",
    "        df['Rolling_Vol'] = df['Daily_Return'].rolling(window=30).std() * np.sqrt(252)\n",
    "        ax1.plot(df.index, df['Rolling_Vol'])\n",
    "        ax1.set_title('30-Day Rolling Volatility')\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Return distribution\n",
    "        df['Daily_Return'].hist(bins=50, ax=ax2)\n",
    "class RiskAnalysis:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def calculate_risk_metrics(self, df):\n",
    "        \"\"\"Calculate various risk metrics\"\"\"\n",
    "        # Volatility metrics\n",
    "        daily_returns = df['Daily_Return']\n",
    "        annual_volatility = daily_returns.std() * np.sqrt(252)\n",
    "        \n",
    "        # Downside risk\n",
    "        downside_returns = daily_returns[daily_returns < 0]\n",
    "        downside_volatility = downside_returns.std() * np.sqrt(252)\n",
    "        \n",
    "        # Value at Risk (VaR)\n",
    "        var_95 = np.percentile(daily_returns, 5)\n",
    "        var_99 = np.percentile(daily_returns, 1)\n",
    "        \n",
    "        # Maximum drawdown\n",
    "        rolling_max = df['Close'].expanding().max()\n",
    "        drawdowns = df['Close']/rolling_max - 1\n",
    "        max_drawdown = drawdowns.min()\n",
    "        \n",
    "        # Sharpe ratio (assuming risk-free rate of 3%)\n",
    "        excess_returns = daily_returns - Config.RISK_FREE_RATE/252\n",
    "        sharpe_ratio = np.sqrt(252) * excess_returns.mean() / excess_returns.std()\n",
    "        \n",
    "        return pd.Series({\n",
    "            'Annual_Volatility': annual_volatility,\n",
    "            'Downside_Volatility': downside_volatility,\n",
    "            'VaR_95': var_95,\n",
    "            'VaR_99': var_99,\n",
    "            'Max_Drawdown': max_drawdown,\n",
    "            'Sharpe_Ratio': sharpe_ratio\n",
    "        })\n",
    "    \n",
    "    def plot_risk_analysis(self, symbol):\n",
    "        \"\"\"Create risk analysis visualizations\"\"\"\n",
    "        df = self.data[symbol]\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Rolling volatility\n",
    "        df['Rolling_Vol'] = df['Daily_Return'].rolling(window=30).std() * np.sqrt(252)\n",
    "        ax1.plot(df.index, df['Rolling_Vol'])\n",
    "        ax1.set_title('30-Day Rolling Volatility')\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Return distribution\n",
    "        df['Daily_Return'].hist(bins=50, ax=ax2)\n",
    "        ax2.set_title('Return Distribution')\n",
    "        \n",
    "        # Drawdown analysis\n",
    "        rolling_max = df['Close'].expanding().max()\n",
    "        drawdowns = df['Close']/rolling_max - 1\n",
    "        ax3.fill_between(df.index, drawdowns, 0, color='red', alpha=0.3)\n",
    "        ax3.set_title('Drawdown Analysis')\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        # Q-Q plot\n",
    "        stats.probplot(df['Daily_Return'].dropna(), dist='norm', plot=ax4)\n",
    "        ax4.set_title('Q-Q Plot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_all_stocks(self):\n",
    "        \"\"\"Perform risk analysis for all stocks\"\"\"\n",
    "        risk_metrics = {}\n",
    "        for symbol in self.data.keys():\n",
    "            print(f\"\\nAnalyzing risk metrics for {symbol}...\")\n",
    "            risk_metrics[symbol] = self.calculate_risk_metrics(self.data[symbol])\n",
    "            self.plot_risk_analysis(symbol)\n",
    "        \n",
    "        return pd.DataFrame(risk_metrics).T\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Assuming you have your stock data loaded into a dictionary called 'processed_data'\n",
    "    # processed_data = {'SYMBOL': pd.DataFrame(...), ...}\n",
    "    \n",
    "    # Initialize and run technical analysis\n",
    "    ta = TechnicalAnalysis(processed_data)\n",
    "    analysis_data = ta.add_technical_indicators()\n",
    "    \n",
    "    # Create visualizations\n",
    "    visualizer = MarketVisualizer(analysis_data)\n",
    "    sample_symbol = list(analysis_data.keys())[0]\n",
    "    visualizer.plot_price_with_indicators(sample_symbol)\n",
    "    visualizer.plot_market_comparison()\n",
    "    visualizer.plot_correlation_heatmap()\n",
    "    \n",
    "    # Perform volume analysis\n",
    "    volume_analyzer = VolumeAnalysis(analysis_data)\n",
    "    volume_stats = volume_analyzer.analyze_all_stocks()\n",
    "    print(\"\\nVolume Analysis Statistics:\")\n",
    "    print(volume_stats.round(2))\n",
    "    \n",
    "    # Perform pattern analysis\n",
    "    pattern_analyzer = PatternAnalysis(analysis_data)\n",
    "    pattern_stats = pattern_analyzer.analyze_all_stocks()\n",
    "    print(\"\\nPattern Analysis Statistics:\")\n",
    "    print(pattern_stats)\n",
    "    \n",
    "    # Perform risk analysis\n",
    "    risk_analyzer = RiskAnalysis(analysis_data)\n",
    "    risk_metrics = risk_analyzer.analyze_all_stocks()\n",
    "    print(\"\\nRisk Analysis Metrics:\")\n",
    "    print(risk_metrics.round(4))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exchange and Sector Analysis Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectorAnalysis:\n",
    "    def __init__(self, data, stock_info):\n",
    "        self.data = data\n",
    "        self.stock_info = stock_info\n",
    "    \n",
    "    def calculate_sector_performance(self):\n",
    "        \"\"\"Calculate performance metrics by sector\"\"\"\n",
    "        sector_returns = {}\n",
    "        sector_metrics = {}\n",
    "        \n",
    "        for exchange in ['SSE', 'SZSE']:\n",
    "            exchange_stocks = self.stock_info[self.stock_info['exchange'] == exchange]\n",
    "            \n",
    "            # Calculate average returns for exchange\n",
    "            exchange_returns = pd.DataFrame()\n",
    "            for symbol in exchange_stocks['symbol']:\n",
    "                if symbol in self.data:\n",
    "                    exchange_returns[symbol] = self.data[symbol]['Daily_Return']\n",
    "            \n",
    "            sector_returns[exchange] = exchange_returns.mean(axis=1)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            total_return = (1 + sector_returns[exchange]).cumprod()[-1] - 1\n",
    "            annualized_return = (1 + total_return) ** (252/len(sector_returns[exchange])) - 1\n",
    "            volatility = sector_returns[exchange].std() * np.sqrt(252)\n",
    "            sharpe = (annualized_return - Config.RISK_FREE_RATE) / volatility\n",
    "            \n",
    "            sector_metrics[exchange] = {\n",
    "                'Total_Return': total_return * 100,\n",
    "                'Annualized_Return': annualized_return * 100,\n",
    "                'Volatility': volatility * 100,\n",
    "                'Sharpe_Ratio': sharpe,\n",
    "                'Number_of_Stocks': len(exchange_stocks)\n",
    "            }\n",
    "        \n",
    "        return pd.DataFrame(sector_metrics).T, sector_returns\n",
    "    \n",
    "    def plot_sector_analysis(self, sector_returns):\n",
    "        \"\"\"Create sector analysis visualizations\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "        \n",
    "        # Cumulative returns comparison\n",
    "        cumulative_returns = {}\n",
    "        for exchange in sector_returns:\n",
    "            cumulative_returns[exchange] = (1 + sector_returns[exchange]).cumprod()\n",
    "            ax1.plot(cumulative_returns[exchange].index, \n",
    "                    cumulative_returns[exchange], \n",
    "                    label=exchange)\n",
    "        \n",
    "        ax1.set_title('Cumulative Returns by Exchange')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Rolling correlation\n",
    "        correlation = pd.DataFrame({\n",
    "            'SSE': sector_returns['SSE'],\n",
    "            'SZSE': sector_returns['SZSE']\n",
    "        }).rolling(window=30).corr().unstack()[1]\n",
    "        \n",
    "        ax2.plot(correlation.index, correlation.values)\n",
    "        ax2.set_title('30-Day Rolling Correlation between Exchanges')\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_exchange_characteristics(self):\n",
    "        \"\"\"Analyze characteristics of each exchange\"\"\"\n",
    "        exchange_stats = {}\n",
    "        \n",
    "        for exchange in ['SSE', 'SZSE']:\n",
    "            exchange_stocks = self.stock_info[self.stock_info['exchange'] == exchange]\n",
    "            \n",
    "            # Calculate market cap statistics\n",
    "            market_cap_stats = exchange_stocks['market_cap'].describe()\n",
    "            \n",
    "            # Calculate average daily volume\n",
    "            volume_data = []\n",
    "            for symbol in exchange_stocks['symbol']:\n",
    "                if symbol in self.data:\n",
    "                    volume_data.append(self.data[symbol]['Volume'].mean())\n",
    "            \n",
    "            avg_volume = np.mean(volume_data) if volume_data else 0\n",
    "            \n",
    "            exchange_stats[exchange] = {\n",
    "                'Total_Market_Cap': exchange_stocks['market_cap'].sum(),\n",
    "                'Average_Market_Cap': market_cap_stats['mean'],\n",
    "                'Median_Market_Cap': market_cap_stats['50%'],\n",
    "                'Largest_Stock': market_cap_stats['max'],\n",
    "                'Smallest_Stock': market_cap_stats['min'],\n",
    "                'Average_Daily_Volume': avg_volume\n",
    "            }\n",
    "        \n",
    "        return pd.DataFrame(exchange_stats).T\n",
    "    \n",
    "    def plot_exchange_composition(self):\n",
    "        \"\"\"Plot the composition of each exchange\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        for i, exchange in enumerate(['SSE', 'SZSE']):\n",
    "            exchange_stocks = self.stock_info[self.stock_info['exchange'] == exchange]\n",
    "            \n",
    "            # Market cap distribution\n",
    "            exchange_stocks['market_cap'].hist(bins=50, ax=ax1 if i == 0 else ax2)\n",
    "            ax1.set_title('SSE Market Cap Distribution')\n",
    "            ax2.set_title('SZSE Market Cap Distribution')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_sectors(self):\n",
    "        \"\"\"Perform comprehensive sector analysis\"\"\"\n",
    "        print(\"Calculating sector performance metrics...\")\n",
    "        metrics, returns = self.calculate_sector_performance()\n",
    "        \n",
    "        print(\"\\nSector Performance Metrics:\")\n",
    "        display(metrics.round(2))\n",
    "        \n",
    "        print(\"\\nPlotting sector analysis...\")\n",
    "        self.plot_sector_analysis(returns)\n",
    "        \n",
    "        print(\"\\nAnalyzing exchange characteristics...\")\n",
    "        characteristics = self.analyze_exchange_characteristics()\n",
    "        \n",
    "        print(\"\\nExchange Characteristics:\")\n",
    "        display(characteristics.round(2))\n",
    "        \n",
    "        print(\"\\nPlotting exchange composition...\")\n",
    "        self.plot_exchange_composition()\n",
    "        \n",
    "        return metrics, characteristics\n",
    "\n",
    "# Add this to the main function from the previous code\n",
    "def main():\n",
    "    # Previous code remains the same...\n",
    "    \n",
    "    # Perform sector analysis (assuming you have stock_info DataFrame)\n",
    "    sector_analyzer = SectorAnalysis(analysis_data, stock_info)\n",
    "    sector_metrics, exchange_characteristics = sector_analyzer.analyze_sectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Analysis Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorAnalysis:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def calculate_size_factor(self, df):\n",
    "        \"\"\"Calculate size factor based on market cap\"\"\"\n",
    "        return np.log(df['Close'] * df['Volume'])\n",
    "    \n",
    "    def calculate_momentum_factor(self, df, lookback=252):\n",
    "        \"\"\"Calculate momentum factor\"\"\"\n",
    "        return df['Close'].pct_change(lookback)\n",
    "    \n",
    "    def calculate_volatility_factor(self, df, window=30):\n",
    "        \"\"\"Calculate volatility factor\"\"\"\n",
    "        return df['Daily_Return'].rolling(window=window).std() * np.sqrt(252)\n",
    "    \n",
    "    def calculate_value_factor(self, df):\n",
    "        \"\"\"Calculate value factor using price trends\"\"\"\n",
    "        return df['Close'] / df['MA_200']\n",
    "    \n",
    "    def compute_factor_returns(self):\n",
    "        \"\"\"Compute returns for each factor\"\"\"\n",
    "        factor_returns = {}\n",
    "        \n",
    "        for symbol, df in self.data.items():\n",
    "            # Calculate factors\n",
    "            factors = pd.DataFrame({\n",
    "                'Size': self.calculate_size_factor(df),\n",
    "                'Momentum': self.calculate_momentum_factor(df),\n",
    "                'Volatility': self.calculate_volatility_factor(df),\n",
    "                'Value': self.calculate_value_factor(df)\n",
    "            })\n",
    "            \n",
    "            # Calculate factor returns\n",
    "            for factor in factors.columns:\n",
    "                # Sort stocks by factor and create portfolios\n",
    "                factor_quintiles = pd.qcut(factors[factor], 5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "                factor_returns[f'{symbol}_{factor}'] = pd.Series(\n",
    "                    df['Daily_Return'].values,\n",
    "                    index=df.index,\n",
    "                    name=factor_quintiles.name\n",
    "                )\n",
    "        \n",
    "        return pd.DataFrame(factor_returns)\n",
    "    \n",
    "    def analyze_factor_exposure(self):\n",
    "        \"\"\"Analyze factor exposures for each stock\"\"\"\n",
    "        factor_exposures = {}\n",
    "        \n",
    "        for symbol, df in self.data.items():\n",
    "            exposures = {\n",
    "                'Size_Exposure': self.calculate_size_factor(df).mean(),\n",
    "                'Momentum_Exposure': self.calculate_momentum_factor(df).mean(),\n",
    "                'Volatility_Exposure': self.calculate_volatility_factor(df).mean(),\n",
    "                'Value_Exposure': self.calculate_value_factor(df).mean()\n",
    "            }\n",
    "            factor_exposures[symbol] = exposures\n",
    "        \n",
    "        return pd.DataFrame(factor_exposures).T\n",
    "    \n",
    "    def plot_factor_analysis(self):\n",
    "        \"\"\"Create factor analysis visualizations\"\"\"\n",
    "        factor_returns = self.compute_factor_returns()\n",
    "        factor_exposures = self.analyze_factor_exposure()\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Factor returns over time\n",
    "        cumulative_returns = (1 + factor_returns).cumprod()\n",
    "        ax1.plot(cumulative_returns.index, cumulative_returns.mean(axis=1))\n",
    "        ax1.set_title('Cumulative Factor Returns')\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Factor exposures heatmap\n",
    "        sns.heatmap(factor_exposures, annot=True, cmap='RdYlBu', center=0, ax=ax2)\n",
    "        ax2.set_title('Factor Exposures by Stock')\n",
    "        \n",
    "        # Factor correlations\n",
    "        factor_corr = factor_returns.corr()\n",
    "        sns.heatmap(factor_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, ax=ax3)\n",
    "        ax3.set_title('Factor Correlations')\n",
    "        \n",
    "        # Factor volatility\n",
    "        factor_vol = factor_returns.std() * np.sqrt(252)\n",
    "        factor_vol.plot(kind='bar', ax=ax4)\n",
    "        ax4.set_title('Factor Volatility (Annualized)')\n",
    "        ax4.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return factor_exposures, factor_returns\n",
    "\n",
    "# Perform factor analysis\n",
    "factor_analyzer = FactorAnalysis(analysis_data)\n",
    "factor_exposures, factor_returns = factor_analyzer.plot_factor_analysis()\n",
    "\n",
    "print(\"\\nFactor Analysis Results:\")\n",
    "print(\"\\nFactor Exposures:\")\n",
    "display(factor_exposures.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Extensions for Stock Market Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class MachineLearningAnalysis:\n",
    "    def __init__(self, processed_data, stock_info):\n",
    "        \"\"\"\n",
    "        Initialize ML analysis with processed stock data\n",
    "        \n",
    "        Args:\n",
    "            processed_data (dict): Dictionary of processed stock DataFrames\n",
    "            stock_info (pd.DataFrame): Information about stocks\n",
    "        \"\"\"\n",
    "        self.data = processed_data\n",
    "        self.stock_info = stock_info\n",
    "        self.features = ['Close', 'Open', 'High', 'Low', 'Volume', \n",
    "                         'Daily_Return', 'Volatility', \n",
    "                         'MA_20', 'MA_50', 'MA_200']\n",
    "    \n",
    "    def prepare_ml_dataset(self, symbol, look_back=30, future_target='Close'):\n",
    "        \"\"\"\n",
    "        Prepare dataset for machine learning models\n",
    "        \n",
    "        Args:\n",
    "            symbol (str): Stock symbol\n",
    "            look_back (int): Number of previous days to use as features\n",
    "            future_target (str): Target variable to predict\n",
    "        \n",
    "        Returns:\n",
    "            X (np.array): Feature matrix\n",
    "            y (np.array): Target variable\n",
    "        \"\"\"\n",
    "        df = self.data[symbol].copy()\n",
    "        \n",
    "        # Select features\n",
    "        feature_data = df[self.features].dropna()\n",
    "        \n",
    "        # Create sliding window features\n",
    "        X, y = [], []\n",
    "        for i in range(len(feature_data) - look_back):\n",
    "            X.append(feature_data.iloc[i:i+look_back].values)\n",
    "            y.append(feature_data.iloc[i+look_back][future_target])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def random_forest_prediction(self, symbol):\n",
    "        \"\"\"\n",
    "        Use Random Forest for stock price prediction\n",
    "        \n",
    "        Args:\n",
    "            symbol (str): Stock symbol\n",
    "        \n",
    "        Returns:\n",
    "            dict: Prediction metrics and model performance\n",
    "        \"\"\"\n",
    "        X, y = self.prepare_ml_dataset(symbol)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "        X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "        \n",
    "        # Random Forest Regressor\n",
    "        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf_model.fit(X_train_scaled.reshape(X_train.shape[0], -1), y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = rf_model.predict(X_test_scaled.reshape(X_test.shape[0], -1))\n",
    "        \n",
    "        # Performance metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        return {\n",
    "            'model': 'Random Forest',\n",
    "            'symbol': symbol,\n",
    "            'RMSE': rmse,\n",
    "            'Predictions': y_pred,\n",
    "            'Actual': y_test\n",
    "        }\n",
    "    \n",
    "    def lstm_price_prediction(self, symbol):\n",
    "        \"\"\"\n",
    "        Use LSTM for stock price prediction\n",
    "        \n",
    "        Args:\n",
    "            symbol (str): Stock symbol\n",
    "        \n",
    "        Returns:\n",
    "            dict: LSTM model performance and predictions\n",
    "        \"\"\"\n",
    "        X, y = self.prepare_ml_dataset(symbol)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "        X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "        \n",
    "        # LSTM Model\n",
    "        model = Sequential([\n",
    "            LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(50, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "        history = model.fit(X_train_scaled, y_train, \n",
    "                            epochs=50, \n",
    "                            batch_size=32, \n",
    "                            validation_split=0.2, \n",
    "                            verbose=0)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test_scaled).flatten()\n",
    "        \n",
    "        # Performance metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        return {\n",
    "            'model': 'LSTM',\n",
    "            'symbol': symbol,\n",
    "            'RMSE': rmse,\n",
    "            'Predictions': y_pred,\n",
    "            'Actual': y_test,\n",
    "            'Training_History': history.history\n",
    "        }\n",
    "    \n",
    "    def stock_clustering(self):\n",
    "        \"\"\"\n",
    "        Perform stock clustering based on features\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Cluster assignments for stocks\n",
    "        \"\"\"\n",
    "        # Aggregate features across stocks\n",
    "        cluster_features = []\n",
    "        for symbol, df in self.data.items():\n",
    "            features = df[self.features].dropna().mean()\n",
    "            features['Symbol'] = symbol\n",
    "            cluster_features.append(features)\n",
    "        \n",
    "        cluster_df = pd.DataFrame(cluster_features)\n",
    "        \n",
    "        # Prepare data for clustering\n",
    "        X_cluster = cluster_df[self.features].values\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_cluster)\n",
    "        \n",
    "        # Apply PCA for dimensionality reduction\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # K-Means Clustering\n",
    "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        # Add cluster labels to DataFrame\n",
    "        cluster_df['Cluster'] = cluster_labels\n",
    "        \n",
    "        # Visualize clusters\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                               c=cluster_labels, \n",
    "                               cmap='viridis')\n",
    "        plt.title('Stock Clustering using K-Means')\n",
    "        plt.xlabel('First Principal Component')\n",
    "        plt.ylabel('Second Principal Component')\n",
    "        plt.colorbar(scatter)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return cluster_df\n",
    "    \n",
    "    def trend_classification(self, symbol):\n",
    "        \"\"\"\n",
    "        Classify stock price trends\n",
    "        \n",
    "        Args:\n",
    "            symbol (str): Stock symbol\n",
    "        \n",
    "        Returns:\n",
    "            dict: Classification performance metrics\n",
    "        \"\"\"\n",
    "        df = self.data[symbol].copy()\n",
    "        \n",
    "        # Create binary trend classification\n",
    "        df['Trend'] = np.where(df['Daily_Return'] > 0, 1, 0)\n",
    "        \n",
    "        # Select features for classification\n",
    "        features = df[self.features].dropna()\n",
    "        target = df['Trend'].dropna()\n",
    "        \n",
    "        # Match feature and target lengths\n",
    "        features = features.iloc[:len(target)]\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Logistic Regression Classifier\n",
    "        lr_model = LogisticRegression()\n",
    "        lr_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = lr_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Performance metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'model': 'Logistic Regression',\n",
    "            'symbol': symbol,\n",
    "            'Accuracy': accuracy,\n",
    "            'Classification_Report': classification_report(y_test, y_pred)\n",
    "        }\n",
    "    \n",
    "    def run_comprehensive_ml_analysis(self):\n",
    "        \"\"\"\n",
    "        Run comprehensive machine learning analysis\n",
    "        \n",
    "        Returns:\n",
    "            dict: Results of various ML analyses\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Perform analyses for each stock\n",
    "        for symbol in self.data.keys():\n",
    "            print(f\"\\nAnalyzing {symbol}...\")\n",
    "            \n",
    "            # Random Forest Prediction\n",
    "            rf_results = self.random_forest_prediction(symbol)\n",
    "            \n",
    "            # LSTM Prediction\n",
    "            lstm_results = self.lstm_price_prediction(symbol)\n",
    "            \n",
    "            # Trend Classification\n",
    "            classification_results = self.trend_classification(symbol)\n",
    "            \n",
    "            results[symbol] = {\n",
    "                'Random_Forest': rf_results,\n",
    "                'LSTM': lstm_results,\n",
    "                'Classification': classification_results\n",
    "            }\n",
    "        \n",
    "        # Perform stock clustering\n",
    "        cluster_results = self.stock_clustering()\n",
    "        results['Clustering'] = cluster_results\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Extend main function to include ML analysis\n",
    "def main():\n",
    "    # Previous code remains the same...\n",
    "    \n",
    "    # Perform ML analysis\n",
    "    ml_analyzer = MachineLearningAnalysis(analysis_data, stock_info)\n",
    "    ml_results = ml_analyzer.run_comprehensive_ml_analysis()\n",
    "    \n",
    "    # Display summary of results\n",
    "    print(\"\\nMachine Learning Analysis Summary:\")\n",
    "    for symbol, results in ml_results.items():\n",
    "        if symbol != 'Clustering':\n",
    "            print(f\"\\n{symbol}:\")\n",
    "            print(f\"Random Forest RMSE: {results['Random_Forest']['RMSE']:.4f}\")\n",
    "            print(f\"LSTM RMSE: {results['LSTM']['RMSE']:.4f}\")\n",
    "            print(f\"Classification Accuracy: {results['Classification']['Accuracy']:.2%}\")\n",
    "    \n",
    "    # Display clustering results\n",
    "    print(\"\\nStock Clustering:\")\n",
    "    print(ml_results['Clustering'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
